{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"colab":{"name":"Copy of AMMI_EXPRL_Exercise.ipynb","provenance":[{"file_id":"1Xz6-c4-b1i0N5QtbdshGnopB1AfPR4cv","timestamp":1582726837782}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_UWM-gGi76pF","colab_type":"code","colab":{}},"source":["!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5EbzJ1A78Bk","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.insert(0, './mvarl_hands_on/exploration')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDPYhYvR7yKZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","from riverswim import RiverSwim\n","import cvxpy as cp\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S5twn7Qv7yKf","colab_type":"text"},"source":["# Finite-Horizon MDPs\n","We consider finite horizon problems with horizon $H$. For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n","\n","The value of a policy or the optimal value function can be computed using *backward induction*.\n","\n","\n","Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n","$$\n","V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n","$$\n","where $V_{H+1}(s) = 0$, for any $s$. "]},{"cell_type":"code","metadata":{"id":"7l8433mz7yKh","colab_type":"code","outputId":"c9ad9008-5bb6-4dea-ae43-f6944b286287","executionInfo":{"status":"ok","timestamp":1582727030768,"user_tz":0,"elapsed":831,"user":{"displayName":"Sewade Ogun","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDEfOB7qezFBx0V72Ki_YNoJEiKqd6Edh8ip6rP=s64","userId":"02961330227233495639"}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["env = RiverSwim(6)\n","H = 10\n","print(\"Reward matrix: \", env.R.shape)\n","print(env.R)\n","print()\n","print(\"Transition matrix: \", env.P.shape)\n","print(\"Transitions probabilities for state s_1:\")\n","print(env.P[1])\n","\n","print(env.P[0, 1, 1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reward matrix:  (6, 2)\n","[[0.005 0.   ]\n"," [0.    0.   ]\n"," [0.    0.   ]\n"," [0.    0.   ]\n"," [0.    0.   ]\n"," [0.    1.   ]]\n","\n","Transition matrix:  (6, 2, 6)\n","Transitions probabilities for state s_1:\n","[[1.   0.   0.   0.   0.   0.  ]\n"," [0.05 0.6  0.35 0.   0.   0.  ]]\n","0.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hiXHdYle7yKm","colab_type":"text"},"source":["# Backward induction (aka Value Iteration)"]},{"cell_type":"code","metadata":{"id":"Q_KZXn8A7yKo","colab_type":"code","colab":{}},"source":["def backward_induction(P, R, H):\n","    \"\"\"\n","        Parameters:\n","            P: transition function (S,A,S)-dim matrix\n","            R: reward function (S,A)-dim matrix\n","            H: horizon\n","\n","        Returns:\n","            The optimal V-function\n","            The optimal policy\n","    \"\"\"\n","    S, A = P.shape[0], P.shape[1]\n","    policy = np.zeros((H, S), dtype=np.int)\n","    V = np.zeros((H + 1, S))\n","    for h in reversed(range(H)):\n","        for s in range(S):\n","            \"\"\" \n","            Here, we compute V^*(h, s) using the Bellman optimality equation:\n","\n","            V[h, s] = max_a  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n","            \"\"\"\n","            for a in range(A):\n","                tmp = np.dot(P[s, a], R[s, a] + V[h + 1])\n","                if (a == 0) or (tmp > V[h, s]):\n","                    policy[h, s] = a\n","                    V[h, s] = tmp\n","    return V, policy\n","\n","def policy_evaluation(P, R, H, policy):\n","    \"\"\"\n","        Parameters:\n","            P: transition function (S,A,S)-dim matrix\n","            R: reward function (S,A)-dim matrix\n","            H: horizon\n","            policy: policy (H,S)-dim matrix\n","\n","        Returns:\n","            The V-function of the given policy\n","    \"\"\"\n","    S, A = P.shape[0], P.shape[1]\n","    V = np.zeros((H + 1, S))\n","    for h in reversed(range(H)):\n","        for s in range(S):\n","            \"\"\" \n","            Here, we compute V^pi(h, s) using the Bellman equation for the policy pi:\n","\n","            a = policy[h,s]\n","            V[h, s] =  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n","            \"\"\"\n","            a = policy[h,s]\n","            # complete the policy evalution here\n","            V[h, s] = \n","    return V"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvmhRCN07yKs","colab_type":"text"},"source":["Compute solution"]},{"cell_type":"code","metadata":{"id":"vVjkydge7yKt","colab_type":"code","colab":{}},"source":["Vstar, POLstar = backward_induction(env.P, env.R, H)\n","\n","print(\"Optimal policy\")\n","print(np.round(Vstar))\n","\n","print(POLstar)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1nCGWuw7yKy","colab_type":"text"},"source":["## UCRL"]},{"cell_type":"markdown","metadata":{"id":"zoWtsh7v7yK0","colab_type":"text"},"source":["UCRL is an algorithm for efficient exploration in finite-horizon tabular MDP.\n","In this setting, the regret is defined as\n","$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n","UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n","\n","The structure of the algorithm is as follow\n","\n","For $k = 1, \\ldots, K$ do<br>\n","> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n","> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",">> for $h=1, \\ldots, H$<br>\n",">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n","\n","<font color='#ed7d31'>Optimistic planning</font>\n","At each episode, UCRL computes the optimal solution by solving the following \"extended\" problem\n","$$\n","V_h^\\star(s) =  \\max_{r \\in B_r(s,a)} r + \\max_{p \\in B_p(s,a)} \\sum_{s'} p(s') V_{h+1}(s') \n","$$\n","where $V_{H+1}(s) = 0$ and $B_r(s,a)$ and $B_p(s,a)$ are confidence intervals on the estimated transitions and rewards:\n","\n","$$\n","B_r(s,a) = \\{ r(s,a):  |r_s, a) - \\hat{r}(s,a)| \\leq \\beta_r(s,a)  \\}\n","$$\n","\n","$$\n","B_p(s,a) = \\{ p(\\cdot|s,a):  ||p(\\cdot|s,a) - \\hat{p}(\\cdot|s,a)||_1 \\leq \\beta_p(s,a)  \\}\n","$$\n","where \n","\n","$$\n","\\beta_r(s, a) = \\sqrt{ \\frac{ \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  } \\\\\n","\\beta_p(s, a) = \\sqrt{ \\frac{  S \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  }\n","$$\n","and where  $N^+(s, a) = \\max(1, N(s, a))$."]},{"cell_type":"markdown","metadata":{"id":"ji7IShLN7yK1","colab_type":"text"},"source":["---\n","The following function computes:\n","$$\\max_{x \\in B_p} \\sum_{s'} x(s') V(s') $$\n","where $B_p = [P-\\beta, P+\\beta]$"]},{"cell_type":"code","metadata":{"id":"57zVp32L7yK3","colab_type":"code","colab":{}},"source":["def LPprobaH(v, P, beta, verbose=0):\n","    \"\"\"\n","        max_x v^T x\n","        s.t.    0 <= x_i <= 1\n","                \\sum_i |x_i - p_i| \\leq beta\n","                \\sum_i x_i = 1\n","    \"\"\"\n","    sorted_idxs = np.argsort(v)[::-1]\n","\n","    pest = P.copy()\n","    idx = sorted_idxs[0]\n","    pest[idx] = min(1., P[idx] + beta / 2.)\n","    delta = pest.sum()\n","    j = len(P)-1\n","    while delta > 1:\n","        idx_j = sorted_idxs[j]\n","        m = max(0, 1. - delta + pest[idx_j])\n","        delta = delta - pest[idx_j] + m\n","        pest[idx_j] = m\n","        j -= 1\n","    w = np.dot(pest, v)\n","    return w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pk0v7jg67yK7","colab_type":"code","colab":{}},"source":["def UCRL(mdp, H, nb_episodes, VSTAR=0):\n","    S, A = mdp.Ns, mdp.Na\n","    policy = np.zeros((H, S), dtype=np.int)\n","    Phat = np.ones((S,A,S)) / S\n","    Rhat = np.zeros((S,A))\n","    N_sas = np.zeros((S,A,S), dtype=np.int)\n","    N_sa = np.zeros((S,A), dtype=np.int)\n","    regret = np.zeros((nb_episodes,))\n","    V = np.zeros((H + 1, S))\n","    \n","    delta = 0.1\n","    \n","    for k in range(nb_episodes):\n","        \n","        # compute optimistic solution\n","        # 1. compute confidence intervals\n","        N = np.maximum(N_sa, 1)\n","        LOGT = np.log(S * A * N / delta)\n","        beta_r = ...\n","        beta_p = ...\n","        \n","        # 2. run extended value iteration\n","        V.fill(0)\n","        for h in reversed(range(H)):\n","            for s in range(S):\n","                for a in range(A):\n","                    dotp = LPprobaH(V[h + 1], Phat[s, a], beta_p[s, a])\n","                    ...\n","        \n","        # execute policy\n","        initial_state = state = mdp.reset()\n","        for h in range(H):\n","            action = policy[h][state]\n","            next_state, reward, done, _ = mdp.step(action)\n","            \n","            # update estimates (Phat, Rhat, N_sa, N_sas)\n","            ...\n","            \n","            state = next_state\n","        \n","        # update regret\n","        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n","        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n","        \n","        if k % 50 == 0:\n","            print(\"regret[{}]: {}\".format(k, regret[k]))\n","    return regret"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-UTr3Gq7yK_","colab_type":"code","colab":{}},"source":["nb_repetitions = 5\n","nb_episodes = 750\n","regrets = np.zeros((nb_repetitions, nb_episodes))\n","for it in range(nb_repetitions):\n","    print(\"Running simulation: {}\".format(it))\n","    regrets[it] = UCRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M49yfZ_S7yLE","colab_type":"code","colab":{}},"source":["x = regrets.cumsum(axis=-1)\n","mean_regret = x.mean(axis=0)\n","std_regret = x.std(axis=0)\n","plt.plot(mean_regret)\n","plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n","plt.ylabel('regret')\n","\n","# SAVE PSRL REGRET\n","ucrl_regret = mean_regret"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myPuG9Er7yLL","colab_type":"text"},"source":["# Posterior Sampling for RL\n","\n","At each iteration, PSRL samples one MDP from the posterior distribution and run the associated optimal policy."]},{"cell_type":"markdown","metadata":{"id":"9gxP6B9r7yLN","colab_type":"text"},"source":["Implement posterior sampling"]},{"cell_type":"code","metadata":{"id":"cl9YwtfJ7yLP","colab_type":"code","colab":{}},"source":["def PSRL(mdp, H, nb_episodes, VSTAR=0):\n","    reward_prior = [1,1]\n","    S, A = mdp.Ns, mdp.Na\n","    Phat = np.ones((S,A,S)) / S\n","    Rhat = np.zeros((S,A))\n","    N_sas = np.zeros((S,A,S), dtype=np.int)\n","    N_sa = np.zeros((S,A), dtype=np.int)\n","    regret = np.zeros((nb_episodes,))\n","    \n","    for k in range(nb_episodes):\n","        \n","        # compute policy\n","        # 1. sample MDP\n","        R = np.zeros_like(Rhat)\n","        P = np.zeros((S, A, S))\n","        for s in range(S):\n","            for a in range(A):\n","                # sample transition matrix\n","                # P[s, a] follows a dirichlet Dirichlet distribution of parameters N_sas[s, a,:] + 1\n","                P[s, a] = ...\n","\n","                # posterior for Bernoulli rewards\n","                N = N_sa[s, a]\n","                v = N * Rhat[s, a]\n","                a0 = reward_prior[0] + v\n","                b0 = reward_prior[1] + N - v\n","                p = np.random.beta(a=a0, b=b0, size=1).item()\n","                R[s, a] = p\n","        \n","        # 2. compute optimal policy\n","        V, policy = ...\n","        \n","        # execute policy\n","        initial_state = state = mdp.reset()\n","        for h in range(H):\n","            action = policy[h][state]\n","            next_state, reward, done, _ = mdp.step(action)\n","            \n","            # update estimates (Rhat, N_sa, N_sas)\n","            ...\n","            \n","            state = next_state\n","        \n","        # update regret\n","        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n","        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n","        \n","        if k % 50 == 0:\n","            print(\"regret[{}]: {}\".format(k, regret[k]))\n","\n","    return regret"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zT38aVOB7yLU","colab_type":"code","colab":{}},"source":["nb_repetitions = 5\n","nb_episodes = 750\n","regrets = np.zeros((nb_repetitions, nb_episodes))\n","for it in range(nb_repetitions):\n","    print(\"Running simulation: {}\".format(it))\n","    regrets[it] = PSRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQhP7cpn7yLY","colab_type":"code","colab":{}},"source":["x = regrets.cumsum(axis=-1)\n","mean_regret = x.mean(axis=0)\n","std_regret = x.std(axis=0)\n","plt.plot(mean_regret)\n","plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n","plt.ylabel('regret')\n","\n","# SAVE PSRL REGRET\n","psrl_regret = mean_regret"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phHZWwdJ7yLc","colab_type":"text"},"source":["Compare algorithms"]},{"cell_type":"code","metadata":{"id":"cvcqwaJY7yLd","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(10,8))\n","plt.plot(ucrl_regret, label='UCRL-H')\n","plt.plot(psrl_regret, label='PSRL')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"it2tQtxL7yLi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}